# Awesome-LLM-Application

## Table of Content
-----

- [Awesome-LLM-Application]
  - [Input UI](#input-ui)
  - [Input Augment](#input-augment)
  - [Model Augment](#model-augment)
  - [RAG-system](#rag-system)
  - [LLM serving](#llm-serving)
  - [LLM evaluate system](#llm-evaluate-system)


## Input UI
----
### Blogs
### Trending Projects
- [Flowise](https://github.com/FlowiseAI/Flowise)
- [chainlit](https://github.com/Chainlit/chainlit)
- [autogen-ui](https://github.com/victordibia/autogen-ui)
- [langui](https://github.com/ahmadbilaldev/langui)
### Companys

## Input Augment
-----
### Blogs
|title|content|updated time|
|-----|-------|------------|
|[使用 Langchain 的 LLM 的对话记忆](https://zhuanlan.zhihu.com/p/639480745)||25/06/2023|

## Model Augment
----

## RAG System
-----
### Blogs
|blog|content|updated time
|-----|------|-------|
|[The architecture of today’s LLM applications](https://github.blog/2023-10-30-the-architecture-of-todays-llm-applications/)||30/10/2023
|[Building RAG-based LLM Applications for Production](https://www.anyscale.com/blog/a-comprehensive-guide-for-building-rag-based-llm-applications-part-1)||25/10/2023
### papers
### Frameworks


### companys

## LLM Serving
-----
### Blogs
|title|content|domain|updated time|
|-----|------|-------|-----------|
|[7 Frameworks for Serving LLMs](https://betterprogramming.pub/frameworks-for-serving-llms-60b7f7b23407)|||31/7/2023|
|[Optimized large language model (LLM) serving
|[大语言模型的模型量化(INT8/INT4)技术](https://zhuanlan.zhihu.com/p/627436535)||quant|6/7/2023|
|
### Trending Frameworks
- [Text Generation Inference](https://huggingface.co/docs/text-generation-inference/index)
- [vLLM](https://github.com/vllm-project/vllm)


### papers
|title|content|domain|conference|data|
|-----|-------|-------|----------|-----|
|[ServerlessLLM: Locality-Enhanced Serverless Inference for Large Language Models](https://arxiv.org/abs/2401.14351)|||25/1/2024|
|[FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness](https://arxiv.org/abs/2205.14135)|||27/05/2023|
|[FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning](https://arxiv.org/abs/2307.08691)||||
|[Efficient memory management for large language model serving with pagedattention](https://dl.acm.org/doi/pdf/10.1145/3600006.3613165)||||



### Frameworks

## LLM Evaluate
-----

## others

### blogs
